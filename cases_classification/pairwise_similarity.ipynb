{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from nltk.stem.snowball import SpanishStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from datetime import datetime\n",
    "import lxml.etree as ET\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import itertools\n",
    "import unidecode\n",
    "import spacy\n",
    "import html\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "INPUT_FILE_PATH\n",
    "- path containing at least the columns: tweet_id, case_id, checked\n",
    "if two different tweet_ids have the same case_id, then both articles are about \n",
    "the same case checked columns indicates whether that \n",
    "    * tweet_id has been explored (1) \n",
    "    * has been randomly assigned a case_id (0)\n",
    "    * has been explored but the answer is not clear (2)\n",
    "    \n",
    "NEWS_PATH\n",
    "- directory where articles are collected in different folders and following the \n",
    "NewsML-G2 format (xml)\n",
    "\n",
    "OUTPUT_PATH\n",
    "- path where to store a csv file containing pairwise similarity metrics and probability\n",
    "of being about the same case for all the news in NEWS_PATH\n",
    "'''\n",
    "\n",
    "INPUT_FILE_PATH = '../data/cases_labeled.csv'\n",
    "NEWS_PATH = '../data/news/'\n",
    "OUTPUT_PATH = '../data/'\n",
    "FASTTEXT_W2V_PATH = 'utilities/embeddings-l-model.vec'\n",
    "\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listdir_checked(path, unwanted = ['.DS_Store']):\n",
    "    '''\n",
    "    Discard unwanted files or directories when listing the elements in a given path\n",
    "    '''\n",
    "    return (f for f in os.listdir(path) if f not in unwanted)\n",
    "\n",
    "\n",
    "def normalize_string(to_normalize, encoded = False):\n",
    "    '''\n",
    "    Normalize text given a string\n",
    "    '''\n",
    "    text = str(to_normalize).lower()  # lowering text\n",
    "    if encoded: \n",
    "        text = ' '.join([html.unescape(term) for term in text.split()])\n",
    "    text = unidecode.unidecode(text)\n",
    "\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # removing all the punctuations\n",
    "    last_text = text.split()  # tokenize the text\n",
    "\n",
    "    # remove stopwords\n",
    "    stopwords_set = set(stopwords.words(\"spanish\"))\n",
    "    last_text = ' '.join([x for x in last_text if (x not in stopwords_set)])\n",
    "    return last_text\n",
    "\n",
    "\n",
    "def normalize_text(array_of_str):\n",
    "    '''\n",
    "    Normalize arrays of strings\n",
    "    '''\n",
    "    final_array = []\n",
    "    for text in array_of_str:\n",
    "        normalized = normalize_string(text)\n",
    "        if normalized != '': final_array.append(normalized)\n",
    "    return final_array\n",
    "\n",
    "\n",
    "def add_similarity_column(pairs_df, mapping_keys, similarity_matrix, column_name):\n",
    "    '''\n",
    "    Arguments:\n",
    "     * pairs_df\n",
    "         - pd.DataFrame\n",
    "         - contains pairs of tweet_ids --> column names: [tweet_id_A, tweet_id_B]\n",
    "     * mapping_keys\n",
    "         - dictionary\n",
    "         - key: tweet_id, value: position\n",
    "     * similarity_matrix\n",
    "         - np.matrix\n",
    "         - symmetrical matrix with the similarity between tweets\n",
    "     * column_name\n",
    "         - string\n",
    "         - name of the new column to be added in pairs_df\n",
    "    '''\n",
    "    similarity_pairs = []               # Create list with the same order as pairs_df\n",
    "    for i, row in pairs_df.iterrows():\n",
    "        tid_A = row['tweet_id_A']       # Obtain tweet id\n",
    "        tid_B = row['tweet_id_B']       # Get the position of each article in the matrix\n",
    "        pos_A = mapping_keys[tid_A]\n",
    "        pos_B = mapping_keys[tid_B]\n",
    "        similarity_pairs.append(similarity_matrix[pos_A, pos_B])             # Order similarity following pairs_df order\n",
    "    pairs_df.insert(len(pairs_df.columns), column_name, similarity_pairs)   # Add new column\n",
    "    return pairs_df\n",
    "\n",
    "\n",
    "def create_articles_dictionary(NEWS_PATH):\n",
    "    '''\n",
    "    Import articles information.\n",
    "    Articles are stored in directories in the NEWS_PATH.\n",
    "    '''\n",
    "    data = {}               # keys: media, value: list of dictionaries with info about the news articles of the given media\n",
    "    unique_urls = []        # list to store unique urls to discard repeated ones\n",
    "    repeated_data = {}      # store repeated articles following the same format as 'data' dictionary\n",
    "\n",
    "    for directory in listdir_checked(NEWS_PATH):\n",
    "        for file in listdir_checked(NEWS_PATH + directory):\n",
    "            full_path = NEWS_PATH + directory + '/' + file\n",
    "            # Read xml file - info stored following NewsML-G2 format\n",
    "            root = ET.parse(full_path).getroot()\n",
    "            # Parse news\n",
    "            media = file.rsplit('_', 1)[0]\n",
    "            # Check repeated urls\n",
    "            url = root.findall(\".//infoSource\")[0].get(\"uri\")\n",
    "            str_date = root.findall('.//contentMeta')[0].find('contentCreated').text[:10]\n",
    "            info = {\n",
    "                'id': file.split(':')[-1].replace('.xml', ''),\n",
    "                'media': media,\n",
    "                'publication_date': datetime.strptime(str_date, '%Y-%m-%d'),\n",
    "                'title': normalize_string(root.findall('.//itemRef')[0].find('title').text, encoded = True),\n",
    "                'headline': normalize_string(root.findall(\".//itemRef\")[0].find('description').text.strip(), encoded = True),\n",
    "                'article': normalize_string(root.findall('.//itemRef')[1].find('description').text.strip(), encoded = True),                'url': url\n",
    "            }\n",
    "\n",
    "            if url not in unique_urls:\n",
    "                unique_urls.append(url)\n",
    "                try:\n",
    "                    data[media].append(info)\n",
    "                except:\n",
    "                    data[media] = [info]\n",
    "\n",
    "            else:\n",
    "                try:\n",
    "                    repeated_data[media].append(info)\n",
    "                except:\n",
    "                    repeated_data[media] = [info]\n",
    "    return data, repeated_data\n",
    "\n",
    "\n",
    "def classify_entities(doc):\n",
    "    '''\n",
    "    Given an nlp doc, returns:\n",
    "    * A dictonary with the entities grouped by type: 'PER', 'LOC', 'ORG' and 'MISC'\n",
    "    * A list with all the entities\n",
    "    '''\n",
    "    classif_dict = {}\n",
    "    classif_list = []\n",
    "    for ent in doc.ents:\n",
    "        try:\n",
    "            classif_list.append(ent)\n",
    "            classif_dict[ent.label_].append(ent)\n",
    "        except:\n",
    "            classif_dict[ent.label_] = [ent]\n",
    "    # Check that the dict has all the keys\n",
    "    for key in ['PER', 'LOC', 'ORG', 'MISC']:\n",
    "        if key not in classif_dict.keys():\n",
    "            classif_dict[key] = None\n",
    "\n",
    "    return classif_dict, classif_list\n",
    "\n",
    "\n",
    "def Goodall1_similarity_matrix(entities_lists, DEBUG=False):\n",
    "    '''\n",
    "    Computes the similarity matrix between different documents given a list of lists.\n",
    "    Each list should contain the categorical variables (strings) that represent the document.\n",
    "    The distance is computed using the Goodall1 measure.\n",
    "\n",
    "    Returns a NxN matrix with similarities (being N the number of documents == length of entities_lists).\n",
    "    '''\n",
    "\n",
    "    # NORMALIZE AND OBTAIN UNIQUE ENTITIES\n",
    "    normalized_lists = [normalize_text(array) for array in entities_lists]\n",
    "    unique_entities = list(set([entity for document in normalized_lists for entity in document]))\n",
    "\n",
    "    N = len(entities_lists)  # number of documents\n",
    "    d = len(unique_entities)  # number of unique_enitities\n",
    "\n",
    "\n",
    "    # COMPUTE SQUARED FREQUENCY FOR EACH ENTITTY\n",
    "    freq_2 = np.zeros(d)  # Squared frequency -> p*p-1 / N*N-1\n",
    "    for i, entity in enumerate(unique_entities):\n",
    "        freq = 0\n",
    "        for document in normalized_lists:\n",
    "            for element in document:\n",
    "                freq += element.count(entity)\n",
    "        freq_2[i] = (freq * (freq - 1)) / (N * (N - 1))\n",
    "\n",
    "    # COMPUTE SIMILARITY MATRIX\n",
    "    similarity_matrix = np.zeros(shape=(N, N))\n",
    "    # Iterate for every pair of documents\n",
    "    for i in range(0, N):\n",
    "        similarity_matrix[i, i] = 1     # put 1 in the diagonal\n",
    "        docA = normalized_lists[i]      # list of normalized entities of document A\n",
    "        for j in range(i + 1, N):\n",
    "            docB = normalized_lists[j]  # list of normalized entities of document B\n",
    "            similarity = 0\n",
    "            # Itereate over each entity\n",
    "            for k in range(d):\n",
    "                entity = unique_entities[k]\n",
    "                if docA.count(entity) > 0 and docB.count(entity) > 0: # if an entity is in both documents, increase similarity\n",
    "                    similarity += (1 / d) * (1 - freq_2[k])\n",
    "            # Fill symmetrical similarity matrix\n",
    "            similarity_matrix[i, j] = similarity\n",
    "            similarity_matrix[j, i] = similarity\n",
    "\n",
    "\n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "def jaccard_coefficient_matrix(entities_lists):\n",
    "    # NORMALIZE THE LIST OF ENTITIES\n",
    "    normalized_lists = [normalize_text(array) for array in entities_lists]\n",
    "    N = len(normalized_lists)\n",
    "\n",
    "    # CREATE SIMILARITY MATRIX\n",
    "    similarity_matrix = np.zeros(shape=(N, N))\n",
    "    # Iterate for every pair of documents\n",
    "    for i in range(N):\n",
    "        similarity_matrix[i, i] = 1\n",
    "        docA = normalized_lists[i]      # list of normalized entities of document A\n",
    "        for j in range(i + 1, N):\n",
    "            docB = normalized_lists[j]  # list of normalized entities of document B\n",
    "            # Compute jaccard similarity\n",
    "            intersection = len(list(set(docA).intersection(docB)))\n",
    "            union = (len(docA) + len(docB)) - intersection\n",
    "            try:\n",
    "                jaccard = float(intersection / union)\n",
    "            except: \n",
    "                jaccard = 0\n",
    "            # Fill similarity matrix\n",
    "            similarity_matrix[i, j] = jaccard\n",
    "            similarity_matrix[j, i] = jaccard\n",
    "\n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "def length_difference_matrix(data, mapping_keys):\n",
    "    N = len(mapping_keys)\n",
    "    article_length = np.zeros(N)\n",
    "    # Get length (number of terms) of each article\n",
    "    for media, new in data.items():\n",
    "        for element in new:\n",
    "            try:\n",
    "                pos = mapping_keys[element['id']]\n",
    "                article_length[pos] = len(element['article'])\n",
    "            except:\n",
    "                pass\n",
    "    # Compute absolute difference of length for each pair of articles\n",
    "    length_diff_matrix = np.zeros(shape=(N, N))\n",
    "    for i in range(N):\n",
    "        for j in range(i + 1, N):\n",
    "            diff = abs(article_length[i] - article_length[j])\n",
    "            length_diff_matrix[i, j] = diff\n",
    "            length_diff_matrix[j, i] = diff\n",
    "\n",
    "    return length_diff_matrix\n",
    "\n",
    "\n",
    "def cosine_similarity_matrix(data, mapping_keys):\n",
    "    # GET NEWS ARTICLES\n",
    "    N = len(mapping_keys)\n",
    "    articles = [None] * N\n",
    "    for media, new in data.items():\n",
    "        for element in new:\n",
    "            try:\n",
    "                pos = mapping_keys[element['id']]\n",
    "                articles[pos] = normalize_string(element['title'] + ' ' + \n",
    "                                                 element['headline'] + ' ' + \n",
    "                                                 element['article'])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    # COMPUTE TF-IDF\n",
    "    # Stem\n",
    "    stemmer = SpanishStemmer(ignore_stopwords=False)\n",
    "    for i, article in enumerate(articles):\n",
    "        articles[i] = str([stemmer.stem(word) for word in article.split()])\n",
    "    # Compute tf-idf\n",
    "    stopwords_spanish = [word.encode().decode('utf-8') for word in stopwords.words('spanish')] # Remove stopwords\n",
    "    vectorizer = TfidfVectorizer(stop_words=stopwords_spanish)\n",
    "    X = vectorizer.fit_transform(articles)\n",
    "\n",
    "    # COMPUTE COSINE SIMILARITY MATRIX\n",
    "    cosine_sim_matrix = np.zeros(shape=(N, N))\n",
    "    for i in range(N):\n",
    "        cosine_sim_matrix[i,i] = 1\n",
    "        for j in range(i + 1, N):\n",
    "            similarity = cosine_similarity(X[i], X[j])[0][0]\n",
    "            cosine_sim_matrix[i, j] = similarity\n",
    "            cosine_sim_matrix[j, i] = similarity\n",
    "\n",
    "    return cosine_sim_matrix\n",
    "\n",
    "\n",
    "def publication_difference_matrix(data, mapping_keys):\n",
    "    N = len(mapping_keys)\n",
    "    publication_dates = [None] * N\n",
    "    # Get publication dates\n",
    "    for media, new in data.items():\n",
    "        for element in new:\n",
    "            try:\n",
    "                pos = mapping_keys[element['id']]\n",
    "                publication_dates[pos] = element['publication_date']\n",
    "            except:\n",
    "                pass\n",
    "    # Compute difference matrix\n",
    "    dates_diff_matrix = np.zeros(shape=(N, N))\n",
    "    for i in range(N):\n",
    "        for j in range(i + 1, N):\n",
    "            diff = abs((publication_dates[i] - publication_dates[j]).days)\n",
    "            dates_diff_matrix[i, j] = diff\n",
    "            dates_diff_matrix[j, i] = diff\n",
    "\n",
    "    return dates_diff_matrix\n",
    "\n",
    "def cosine_similarity_BERT(data, mapping_keys, key='title'):                          \n",
    "    N = len(mapping_keys)\n",
    "    articles = [None] * N\n",
    "    for media, new in data.items():\n",
    "        for element in new:\n",
    "            try:\n",
    "                pos = mapping_keys[element['id']]\n",
    "                #title = normalize_string(element['title'])\n",
    "                #description = normalize_string(element['headline'])\n",
    "                articles[pos] = normalize_string(element[key])\n",
    "            except:\n",
    "                pass\n",
    "   \n",
    "    BERT_model = SentenceTransformer('dccuchile/bert-base-spanish-wwm-uncased')\n",
    "    encoded = BERT_model.encode(articles)\n",
    "    cosine_sim_matrix = np.zeros(shape=(N, N))\n",
    "\n",
    "    for i in range(N):\n",
    "        cosine_sim_matrix[i,i] = 1\n",
    "        for j in range(i + 1, N):\n",
    "    \n",
    "            similarity = cosine_similarity(encoded[i].reshape(1,-1), encoded[j].reshape(1,-1))[0][0]\n",
    "            cosine_sim_matrix[i, j] = similarity\n",
    "            cosine_sim_matrix[j, i] = similarity\n",
    "                          \n",
    "    return cosine_sim_matrix \n",
    "\n",
    "def wmdistance(data, mapping_keys):\n",
    "    N = len(mapping_keys)\n",
    "    articles = [None] * N\n",
    "    \n",
    "    for media, new in data.items():\n",
    "        for element in new:\n",
    "            try:\n",
    "                pos = mapping_keys[element['id']]\n",
    "                #title = normalize_string(element['title'])\n",
    "                #description = normalize_string(element['headline'])\n",
    "                articles[pos] = normalize_string(element['title'])+' '+normalize_string(element['headline'].split('.')[0])\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    wordvectors_fasttext_file = FASTTEXT_W2V_PATH\n",
    "    wordvectors = KeyedVectors.load_word2vec_format(wordvectors_fasttext_file)\n",
    "    wmd_matrix = np.zeros(shape=(N,N))\n",
    "    \n",
    "    for i in range(N):\n",
    "        for j in range(i + 1, N):\n",
    "            distance = wordvectors.wmdistance(articles[i], articles[j])\n",
    "            wmd_matrix[i,j] = distance\n",
    "            wmd_matrix[j,i] = distance\n",
    "            \n",
    "    return wmd_matrix\n",
    "    \n",
    "\n",
    "def train_log_reg(df, id_columns, target_column):\n",
    "    # split training / test data\n",
    "    df = df.drop(columns=id_columns)\n",
    "    training_data, testing_data = train_test_split(df, random_state=2000, test_size=0.1)\n",
    "    # get labels\n",
    "    Y_train = training_data[target_column].values\n",
    "    Y_test = testing_data[target_column].values\n",
    "\n",
    "    X_train = training_data.drop(columns=[target_column])\n",
    "    X_test = testing_data.drop(columns=[target_column])\n",
    "\n",
    "    # logistic regression classifier\n",
    "    scikit_log_reg = LogisticRegression(verbose=1, solver='liblinear', C=5, penalty='l2', max_iter=1000)\n",
    "\n",
    "    model = scikit_log_reg.fit(X_train, Y_train.astype(int))\n",
    "    score = model.score(X_test, Y_test.astype(int))\n",
    "    print(\"Testing accuracy of LogReg = \", score)\n",
    "    return (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(data, cases_df, pairs_df):\n",
    "    # EXTRACT ENTITIES\n",
    "    '''\n",
    "    Entities will be stored two arrays:\n",
    "    * entities - all entities\n",
    "    * summary_entities - entities from the title and head (summary)\n",
    "    Auxiliary variables:\n",
    "    * mapping_keys - dict with key: tweet id -> value: absolute position in entities and summary_entities\n",
    "    '''\n",
    "    mapping_keys = {}  # key: tweet id -> value: absolute position in all_entities\n",
    "    counter = 0\n",
    "\n",
    "    summary_entities = []\n",
    "    article_entities = []\n",
    "\n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "    for media, new in data.items():\n",
    "        for element in new:\n",
    "            mapping_keys[element['id']] = counter\n",
    "            # Get entities from each part of the new\n",
    "            try:\n",
    "                title_doc = nlp(element['title'])\n",
    "                headline_doc = nlp(element['headline'])\n",
    "                article_doc = nlp(element['article'])\n",
    "            except:\n",
    "                if DEBUG:\n",
    "                    print(f\"Problem extracting entities from article identified with tweet_id = {element['id']}\")\n",
    "                pass\n",
    "                          \n",
    "            # Classify entities\n",
    "            title_dict, title_list = classify_entities(title_doc)\n",
    "            headline_dict, headline_list = classify_entities(headline_doc)\n",
    "            article_dict, article_list = classify_entities(article_doc)\n",
    "            \n",
    "            # Add entities into data\n",
    "            element['title_entities'] = title_dict\n",
    "            element['headline_entities'] = headline_dict\n",
    "            element['article_entities'] = article_dict\n",
    "            \n",
    "            # Store into entities and summary_entities array\n",
    "            summary_entities.append(title_list + headline_list)\n",
    "            article_entities.append(article_list)\n",
    "            \n",
    "            counter += 1\n",
    "    \n",
    "         \n",
    "    # COMPUTE GOODALL1 SIMILARITY BETWEEN ENTITIES IN THE SUMMARY (title + summary) OF EACH PAIR OF ARTICLE\n",
    "    goodall1_matrix = Goodall1_similarity_matrix(summary_entities)\n",
    "    pairs_df = add_similarity_column(pairs_df, mapping_keys, goodall1_matrix, 'goodall1_summary')\n",
    "\n",
    "    # COMPUTE GOODALL1 SIMILARITY BETWEEN ENTITIES IN THE ARTICLE  OF EACH PAIR OF ARTICLE\n",
    "    goodall1_matrix = Goodall1_similarity_matrix(article_entities)\n",
    "    pairs_df = add_similarity_column(pairs_df, mapping_keys, goodall1_matrix, 'goodall1_article')\n",
    "\n",
    "                          \n",
    "    # COMPUTE JACCARD COEFFICIENT BETWEEN SUMMARY ENTITIES OF EACH PAIR OF ARTICLES\n",
    "    jaccard_matrix = jaccard_coefficient_matrix(summary_entities)\n",
    "    pairs_df = add_similarity_column(pairs_df, mapping_keys, jaccard_matrix, 'jaccard_summary')\n",
    "                                     \n",
    "    # COMPUTE JACCARD COEFFICIENT BETWEEN ALL ENTITIES OF EACH PAIR OF ARTICLES\n",
    "    jaccard_matrix = jaccard_coefficient_matrix(article_entities)\n",
    "    pairs_df = add_similarity_column(pairs_df, mapping_keys, jaccard_matrix, 'jaccard_article')\n",
    "\n",
    "                          \n",
    "    # COMPUTE COSINE SIMILARITY OF TF-IDF FOR EACH PAIR OF ARTICLES\n",
    "    cosine_matrix = cosine_similarity_matrix(data, mapping_keys) \n",
    "    pairs_df = add_similarity_column(pairs_df, mapping_keys, cosine_matrix, 'tf-idf')\n",
    "     \n",
    "                          \n",
    "    # COMPUTE COSINE SIMILARITY OF BERT WORD EMBEDDING\n",
    "    BERT_similarity_t = cosine_similarity_BERT(data, mapping_keys)\n",
    "    pairs_df = add_similarity_column(pairs_df, mapping_keys, BERT_similarity_t, 'BETO_title')\n",
    "\n",
    "                          \n",
    "    # COMPUTE WORD MOVER'S DISTANCE OF FASTTEXT EMBEDDINGS OF TITLES\n",
    "    wmd_matrix = wmdistance(data, mapping_keys)\n",
    "    pairs_df = add_similarity_column(pairs_df, mapping_keys, wmd_matrix, 'word_movers_dist')\n",
    "          \n",
    "                          \n",
    "    # COMPUTE ABSOLUTE DIFFERENCE PUBLICATION DATES IN DAYS\n",
    "    publication_matrix = publication_difference_matrix(data, mapping_keys)\n",
    "    pairs_df = add_similarity_column(pairs_df, mapping_keys, publication_matrix, 'publication_diff')\n",
    "    \n",
    "    # COMPUTE ABSOLUTE DIFFERENCE OF ARTICLES LENGTH BETWEEN EACH PAIR OF ARTICLES\n",
    "    length_matrix = length_difference_matrix(data, mapping_keys)\n",
    "    pairs_df = add_similarity_column(pairs_df, mapping_keys, length_matrix, 'length_diff')\n",
    "\n",
    "    return pairs_df\n",
    "                          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data and create pairwise features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT ARTICLES INFORMATION\n",
    "data, repeated_data = create_articles_dictionary(NEWS_PATH)\n",
    "\n",
    "if DEBUG:\n",
    "    for media, news in repeated_data.items():\n",
    "        print(f'''* Media {media} has {len(news)} repeated elements.''')\n",
    "\n",
    "# IMPORT LABELS FROM CSV FILE\n",
    "'''\n",
    "Csv file containing information about each article using their corresponding tweet_id as an identifier.\n",
    "Columns = [tweet_id, media, checked, case_id, title, headline, url]\n",
    "* tweet_id - identifier\n",
    "* media\n",
    "* checked - whether the tweet_id has been supervised - values: 0/1\n",
    "* case_id - an integer id attributed to each article, articles about the same case have the same case_id\n",
    "* title\n",
    "* headline\n",
    "* url \n",
    "'''\n",
    "cases_df = pd.read_csv(INPUT_FILE_PATH, sep=';')\n",
    "cases_df['tweet_id'] = cases_df['tweet_id'].apply(lambda x: str(x))\n",
    "\n",
    "# Get tweet ids of tweets not supervised\n",
    "if 'checked' in cases_df.keys():\n",
    "    checked_tweet_id = cases_df[cases_df['checked'] == 1]['tweet_id'].values\n",
    "\n",
    "# Remove repeated articles\n",
    "repeated_articles_tid = []\n",
    "for media, news in repeated_data.items():\n",
    "    for element in news:\n",
    "        repeated_articles_tid.append(element['id'])\n",
    "cases_df = cases_df[~cases_df['tweet_id'].isin(repeated_articles_tid)]\n",
    "\n",
    "# CREATE ALL POSSIBLE PAIRS OF ARTICLES + ADD TARGET VARIABLE (same_case)\n",
    "pairs_df = pd.DataFrame(columns=['tweet_id_A', 'tweet_id_B', 'same_case'])\n",
    "for pair in list(itertools.combinations(cases_df.tweet_id, 2)):\n",
    "    case_1 = int(cases_df[cases_df['tweet_id'] == pair[0]]['case_id'])\n",
    "    case_2 = int(cases_df[cases_df['tweet_id'] == pair[1]]['case_id'])\n",
    "\n",
    "    if case_1 == case_2:\n",
    "        same_case = 1\n",
    "    else:\n",
    "        same_case = 0\n",
    "\n",
    "    pairs_df = pairs_df.append({'tweet_id_A': pair[0], 'tweet_id_B': pair[1], 'same_case': same_case}, ignore_index=True)\n",
    "\n",
    "if DEBUG:\n",
    "    print(f\"{(pairs_df['same_case'] == 1).sum() / len(pairs_df) * 100}% of positive cases\")\n",
    "    print(f\"{(pairs_df['same_case'] == 1).sum()} / {len(pairs_df)}: same cases / total # pairs\")\n",
    "    print(f\"{len(cases_df)} total number of tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CREATE FEATURES\n",
    "pairs_df = create_features(data, cases_df, pairs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id_A</th>\n",
       "      <th>tweet_id_B</th>\n",
       "      <th>same_case</th>\n",
       "      <th>goodall1_summary</th>\n",
       "      <th>goodall1_article</th>\n",
       "      <th>jaccard_summary</th>\n",
       "      <th>jaccard_article</th>\n",
       "      <th>tf-idf</th>\n",
       "      <th>BETO_title</th>\n",
       "      <th>word_movers_dist</th>\n",
       "      <th>publication_diff</th>\n",
       "      <th>length_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1287463951375179776</td>\n",
       "      <td>1288437861973471232</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.039036</td>\n",
       "      <td>0.740956</td>\n",
       "      <td>0.826905</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2570.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1287463951375179776</td>\n",
       "      <td>1288541048520744962</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.113098</td>\n",
       "      <td>0.740605</td>\n",
       "      <td>0.930301</td>\n",
       "      <td>3.0</td>\n",
       "      <td>428.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1287463951375179776</td>\n",
       "      <td>1288632903275106304</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017701</td>\n",
       "      <td>0.605755</td>\n",
       "      <td>0.803036</td>\n",
       "      <td>3.0</td>\n",
       "      <td>251.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1287463951375179776</td>\n",
       "      <td>1288919794729836544</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.036161</td>\n",
       "      <td>0.694141</td>\n",
       "      <td>0.842197</td>\n",
       "      <td>4.0</td>\n",
       "      <td>887.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1287463951375179776</td>\n",
       "      <td>1288764366863708161</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.030533</td>\n",
       "      <td>0.711093</td>\n",
       "      <td>0.885106</td>\n",
       "      <td>4.0</td>\n",
       "      <td>920.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122755</th>\n",
       "      <td>1303099478501666818</td>\n",
       "      <td>1303718556308303878</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060689</td>\n",
       "      <td>0.709489</td>\n",
       "      <td>0.860457</td>\n",
       "      <td>2.0</td>\n",
       "      <td>451.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122756</th>\n",
       "      <td>1303099478501666818</td>\n",
       "      <td>1303696679800012801</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.051923</td>\n",
       "      <td>0.718233</td>\n",
       "      <td>0.781999</td>\n",
       "      <td>2.0</td>\n",
       "      <td>296.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122757</th>\n",
       "      <td>1304387150507651072</td>\n",
       "      <td>1303718556308303878</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041908</td>\n",
       "      <td>0.684583</td>\n",
       "      <td>0.788712</td>\n",
       "      <td>2.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122758</th>\n",
       "      <td>1304387150507651072</td>\n",
       "      <td>1303696679800012801</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.121317</td>\n",
       "      <td>0.687701</td>\n",
       "      <td>0.806449</td>\n",
       "      <td>2.0</td>\n",
       "      <td>127.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122759</th>\n",
       "      <td>1303718556308303878</td>\n",
       "      <td>1303696679800012801</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.042194</td>\n",
       "      <td>0.698578</td>\n",
       "      <td>1.019945</td>\n",
       "      <td>0.0</td>\n",
       "      <td>155.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>122760 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tweet_id_A           tweet_id_B same_case  goodall1_summary  \\\n",
       "0       1287463951375179776  1288437861973471232         0               0.0   \n",
       "1       1287463951375179776  1288541048520744962         0               0.0   \n",
       "2       1287463951375179776  1288632903275106304         0               0.0   \n",
       "3       1287463951375179776  1288919794729836544         0               0.0   \n",
       "4       1287463951375179776  1288764366863708161         0               0.0   \n",
       "...                     ...                  ...       ...               ...   \n",
       "122755  1303099478501666818  1303718556308303878         0               0.0   \n",
       "122756  1303099478501666818  1303696679800012801         0               0.0   \n",
       "122757  1304387150507651072  1303718556308303878         0               0.0   \n",
       "122758  1304387150507651072  1303696679800012801         0               0.0   \n",
       "122759  1303718556308303878  1303696679800012801         0               0.0   \n",
       "\n",
       "        goodall1_article  jaccard_summary  jaccard_article    tf-idf  \\\n",
       "0                    0.0              0.0              0.0  0.039036   \n",
       "1                    0.0              0.0              0.0  0.113098   \n",
       "2                    0.0              0.0              0.0  0.017701   \n",
       "3                    0.0              0.0              0.0  0.036161   \n",
       "4                    0.0              0.0              0.0  0.030533   \n",
       "...                  ...              ...              ...       ...   \n",
       "122755               0.0              0.0              0.0  0.060689   \n",
       "122756               0.0              0.0              0.0  0.051923   \n",
       "122757               0.0              0.0              0.0  0.041908   \n",
       "122758               0.0              0.0              0.0  0.121317   \n",
       "122759               0.0              0.0              0.0  0.042194   \n",
       "\n",
       "        BETO_title  word_movers_dist  publication_diff  length_diff  \n",
       "0         0.740956          0.826905               4.0       2570.0  \n",
       "1         0.740605          0.930301               3.0        428.0  \n",
       "2         0.605755          0.803036               3.0        251.0  \n",
       "3         0.694141          0.842197               4.0        887.0  \n",
       "4         0.711093          0.885106               4.0        920.0  \n",
       "...            ...               ...               ...          ...  \n",
       "122755    0.709489          0.860457               2.0        451.0  \n",
       "122756    0.718233          0.781999               2.0        296.0  \n",
       "122757    0.684583          0.788712               2.0         28.0  \n",
       "122758    0.687701          0.806449               2.0        127.0  \n",
       "122759    0.698578          1.019945               0.0        155.0  \n",
       "\n",
       "[122760 rows x 12 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a logistic regression model \n",
    "GOAL: Identify pairs of news articles about the same case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]Testing accuracy of LogReg =  0.9983356449375866\n"
     ]
    }
   ],
   "source": [
    "# TRAIN A LOGISTIC REGRESSION CLASSIFIER\n",
    "# Filter only checked cases for training\n",
    "supervised_pairs_df = pairs_df[pairs_df['same_case'] != 2]  # 2 has been used for ambiguous cases\n",
    "supervised_pairs_df = supervised_pairs_df[supervised_pairs_df['tweet_id_A'].isin(checked_tweet_id)]\n",
    "supervised_pairs_df = supervised_pairs_df[supervised_pairs_df['tweet_id_B'].isin(checked_tweet_id)]\n",
    "\n",
    "# Train the model\n",
    "log_reg_model = train_log_reg(supervised_pairs_df, id_columns=['tweet_id_A', 'tweet_id_B'], target_column='same_case')\n",
    "\n",
    "# PREDICT FOR ALL PAIRS OF NEWS\n",
    "prediction = log_reg_model.predict(pairs_df.drop(columns=['tweet_id_A', 'tweet_id_B', 'same_case']))\n",
    "proba_prediction = log_reg_model.predict_proba(pairs_df.drop(columns=['tweet_id_A', 'tweet_id_B', 'same_case']))[:,1]\n",
    "pairs_df.insert(len(pairs_df.columns), 'same_case_pred', prediction)\n",
    "pairs_df.insert(len(pairs_df.columns), 'same_case_pred_proba', proba_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id_A</th>\n",
       "      <th>tweet_id_B</th>\n",
       "      <th>same_case</th>\n",
       "      <th>goodall1_summary</th>\n",
       "      <th>goodall1_article</th>\n",
       "      <th>jaccard_summary</th>\n",
       "      <th>jaccard_article</th>\n",
       "      <th>tf-idf</th>\n",
       "      <th>BETO_title</th>\n",
       "      <th>word_movers_dist</th>\n",
       "      <th>publication_diff</th>\n",
       "      <th>length_diff</th>\n",
       "      <th>same_case_pred</th>\n",
       "      <th>same_case_pred_proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1287463951375179776</td>\n",
       "      <td>1288437861973471232</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.039036</td>\n",
       "      <td>0.740956</td>\n",
       "      <td>0.826905</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2570.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1287463951375179776</td>\n",
       "      <td>1288541048520744962</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.113098</td>\n",
       "      <td>0.740605</td>\n",
       "      <td>0.930301</td>\n",
       "      <td>3.0</td>\n",
       "      <td>428.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1287463951375179776</td>\n",
       "      <td>1288632903275106304</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017701</td>\n",
       "      <td>0.605755</td>\n",
       "      <td>0.803036</td>\n",
       "      <td>3.0</td>\n",
       "      <td>251.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1287463951375179776</td>\n",
       "      <td>1288919794729836544</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.036161</td>\n",
       "      <td>0.694141</td>\n",
       "      <td>0.842197</td>\n",
       "      <td>4.0</td>\n",
       "      <td>887.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1287463951375179776</td>\n",
       "      <td>1288764366863708161</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.030533</td>\n",
       "      <td>0.711093</td>\n",
       "      <td>0.885106</td>\n",
       "      <td>4.0</td>\n",
       "      <td>920.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122755</th>\n",
       "      <td>1303099478501666818</td>\n",
       "      <td>1303718556308303878</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060689</td>\n",
       "      <td>0.709489</td>\n",
       "      <td>0.860457</td>\n",
       "      <td>2.0</td>\n",
       "      <td>451.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122756</th>\n",
       "      <td>1303099478501666818</td>\n",
       "      <td>1303696679800012801</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.051923</td>\n",
       "      <td>0.718233</td>\n",
       "      <td>0.781999</td>\n",
       "      <td>2.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122757</th>\n",
       "      <td>1304387150507651072</td>\n",
       "      <td>1303718556308303878</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041908</td>\n",
       "      <td>0.684583</td>\n",
       "      <td>0.788712</td>\n",
       "      <td>2.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122758</th>\n",
       "      <td>1304387150507651072</td>\n",
       "      <td>1303696679800012801</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.121317</td>\n",
       "      <td>0.687701</td>\n",
       "      <td>0.806449</td>\n",
       "      <td>2.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122759</th>\n",
       "      <td>1303718556308303878</td>\n",
       "      <td>1303696679800012801</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.042194</td>\n",
       "      <td>0.698578</td>\n",
       "      <td>1.019945</td>\n",
       "      <td>0.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>122760 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tweet_id_A           tweet_id_B same_case  goodall1_summary  \\\n",
       "0       1287463951375179776  1288437861973471232         0               0.0   \n",
       "1       1287463951375179776  1288541048520744962         0               0.0   \n",
       "2       1287463951375179776  1288632903275106304         0               0.0   \n",
       "3       1287463951375179776  1288919794729836544         0               0.0   \n",
       "4       1287463951375179776  1288764366863708161         0               0.0   \n",
       "...                     ...                  ...       ...               ...   \n",
       "122755  1303099478501666818  1303718556308303878         0               0.0   \n",
       "122756  1303099478501666818  1303696679800012801         0               0.0   \n",
       "122757  1304387150507651072  1303718556308303878         0               0.0   \n",
       "122758  1304387150507651072  1303696679800012801         0               0.0   \n",
       "122759  1303718556308303878  1303696679800012801         0               0.0   \n",
       "\n",
       "        goodall1_article  jaccard_summary  jaccard_article    tf-idf  \\\n",
       "0                    0.0              0.0              0.0  0.039036   \n",
       "1                    0.0              0.0              0.0  0.113098   \n",
       "2                    0.0              0.0              0.0  0.017701   \n",
       "3                    0.0              0.0              0.0  0.036161   \n",
       "4                    0.0              0.0              0.0  0.030533   \n",
       "...                  ...              ...              ...       ...   \n",
       "122755               0.0              0.0              0.0  0.060689   \n",
       "122756               0.0              0.0              0.0  0.051923   \n",
       "122757               0.0              0.0              0.0  0.041908   \n",
       "122758               0.0              0.0              0.0  0.121317   \n",
       "122759               0.0              0.0              0.0  0.042194   \n",
       "\n",
       "        BETO_title  word_movers_dist  publication_diff  length_diff  \\\n",
       "0         0.740956          0.826905               4.0       2570.0   \n",
       "1         0.740605          0.930301               3.0        428.0   \n",
       "2         0.605755          0.803036               3.0        251.0   \n",
       "3         0.694141          0.842197               4.0        887.0   \n",
       "4         0.711093          0.885106               4.0        920.0   \n",
       "...            ...               ...               ...          ...   \n",
       "122755    0.709489          0.860457               2.0        451.0   \n",
       "122756    0.718233          0.781999               2.0        296.0   \n",
       "122757    0.684583          0.788712               2.0         28.0   \n",
       "122758    0.687701          0.806449               2.0        127.0   \n",
       "122759    0.698578          1.019945               0.0        155.0   \n",
       "\n",
       "        same_case_pred  same_case_pred_proba  \n",
       "0                    0              0.001117  \n",
       "1                    0              0.003478  \n",
       "2                    0              0.000501  \n",
       "3                    0              0.000820  \n",
       "4                    0              0.000705  \n",
       "...                ...                   ...  \n",
       "122755               0              0.001279  \n",
       "122756               0              0.001188  \n",
       "122757               0              0.000897  \n",
       "122758               0              0.004441  \n",
       "122759               0              0.000672  \n",
       "\n",
       "[122760 rows x 14 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_df.to_csv(f'{OUTPUT_DATA}/cases_pariwise_proba.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
